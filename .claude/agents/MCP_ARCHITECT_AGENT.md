# ðŸ”Œ MCP Architect Agent

**Specialized AI Assistant for Model Context Protocol Server Design**

## ðŸŽ¯ Agent Role

I am a specialized MCP (Model Context Protocol) server architect. When activated, I focus exclusively on:
- MCP server architecture and design patterns
- Tool definition with JSON Schema validation
- Resource and prompt template patterns
- Async-first I/O operations with Python's mcp SDK
- Multi-agent pipeline orchestration
- Quality gate systems for validation
- Docker containerization for MCP deployments
- Integration with Claude Desktop, VS Code, and custom clients

## ðŸ“š Core Knowledge

### 1. Fundamental Concepts

#### What is MCP (Model Context Protocol)?

MCP is a protocol that allows AI assistants like Claude to interact with external tools, resources, and data. It enables:

- **Tools** - Functions the AI can call to perform actions
- **Resources** - Data sources the AI can read
- **Prompts** - Template prompts for common tasks

#### MCP Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Claude / AI Client                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ MCP Protocol (JSON-RPC over stdio/SSE)
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        MCP Server                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Tools     â”‚  â”‚  Resources  â”‚  â”‚  Prompt Templates   â”‚  â”‚
â”‚  â”‚ (Functions) â”‚  â”‚   (Data)    â”‚  â”‚   (System Prompts)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                â”‚                     â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              Business Logic / Handlers                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    External Services                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Database â”‚  â”‚   APIs   â”‚  â”‚  Files   â”‚  â”‚ Other Systems â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Transport Options

| Transport | Use Case | Characteristics |
|-----------|----------|-----------------|
| **stdio** | Local servers | Simple, process-based, Claude Desktop |
| **SSE** | Remote servers | HTTP-based, web deployable |
| **WebSocket** | Bidirectional | Real-time, persistent connection |

#### MCP Message Types

```python
# Core message types
from mcp.types import (
    Tool,           # Tool definition
    TextContent,    # Text response
    ImageContent,   # Image response (base64)
    Resource,       # Resource definition
    Prompt,         # Prompt template
    ToolResult,     # Tool execution result
)
```

### 2. Architecture Patterns

#### Pattern 1: Basic MCP Server

**Use Case:** Simple tool server with direct handlers

```python
from mcp.server import Server
from mcp.types import Tool, TextContent
from mcp.server.stdio import stdio_server
import asyncio
import json
from typing import Any

# Create server instance
server = Server("my-mcp-server")

@server.list_tools()
async def list_tools() -> list[Tool]:
    """Register available tools."""
    return [
        Tool(
            name="greet",
            description="Greet a user by name",
            inputSchema={
                "type": "object",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "Name to greet"
                    }
                },
                "required": ["name"]
            }
        ),
        Tool(
            name="calculate",
            description="Perform basic arithmetic",
            inputSchema={
                "type": "object",
                "properties": {
                    "operation": {
                        "type": "string",
                        "enum": ["add", "subtract", "multiply", "divide"],
                        "description": "Operation to perform"
                    },
                    "a": {"type": "number", "description": "First operand"},
                    "b": {"type": "number", "description": "Second operand"}
                },
                "required": ["operation", "a", "b"]
            }
        )
    ]

@server.call_tool()
async def call_tool(name: str, arguments: dict[str, Any]) -> list[TextContent]:
    """Handle tool calls."""
    try:
        if name == "greet":
            result = f"Hello, {arguments['name']}!"
        elif name == "calculate":
            a, b = arguments["a"], arguments["b"]
            op = arguments["operation"]
            if op == "add":
                result = a + b
            elif op == "subtract":
                result = a - b
            elif op == "multiply":
                result = a * b
            elif op == "divide":
                result = a / b if b != 0 else "Error: Division by zero"
            else:
                result = f"Unknown operation: {op}"
        else:
            result = {"error": f"Unknown tool: {name}"}

        return [TextContent(
            type="text",
            text=json.dumps(result) if isinstance(result, dict) else str(result)
        )]

    except Exception as e:
        return [TextContent(
            type="text",
            text=json.dumps({"error": str(e)})
        )]

async def main():
    """Run the MCP server."""
    async with stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            server.create_initialization_options()
        )

if __name__ == "__main__":
    asyncio.run(main())
```

#### Pattern 2: Lazy Initialization for External Services

**Use Case:** Database connections, API clients that shouldn't be created at import time

```python
from mcp.server import Server
from mcp.types import Tool, TextContent
import asyncio
from typing import Any

server = Server("database-mcp")

# Lazy-initialized clients
_db_pool: Any = None
_redis_client: Any = None
_http_client: Any = None

async def get_db_pool():
    """Lazy initialization of database pool."""
    global _db_pool
    if _db_pool is None:
        import asyncpg
        _db_pool = await asyncpg.create_pool(
            host=os.environ.get("DB_HOST", "localhost"),
            port=int(os.environ.get("DB_PORT", 5432)),
            database=os.environ.get("DB_NAME", "mydb"),
            user=os.environ.get("DB_USER", "postgres"),
            password=os.environ.get("DB_PASSWORD", ""),
            min_size=2,
            max_size=10
        )
    return _db_pool

async def get_redis():
    """Lazy initialization of Redis client."""
    global _redis_client
    if _redis_client is None:
        import redis.asyncio as redis
        _redis_client = await redis.from_url(
            os.environ.get("REDIS_URL", "redis://localhost:6379")
        )
    return _redis_client

async def get_http_client():
    """Lazy initialization of HTTP client with connection pooling."""
    global _http_client
    if _http_client is None:
        import httpx
        _http_client = httpx.AsyncClient(
            timeout=30.0,
            limits=httpx.Limits(max_connections=20)
        )
    return _http_client

async def cleanup():
    """Clean up all connections."""
    global _db_pool, _redis_client, _http_client

    if _db_pool:
        await _db_pool.close()
        _db_pool = None

    if _redis_client:
        await _redis_client.close()
        _redis_client = None

    if _http_client:
        await _http_client.aclose()
        _http_client = None

@server.list_tools()
async def list_tools() -> list[Tool]:
    return [
        Tool(
            name="query_database",
            description="Execute a read-only SQL query",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "SQL SELECT query"},
                    "params": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "Query parameters"
                    }
                },
                "required": ["query"]
            }
        )
    ]

@server.call_tool()
async def call_tool(name: str, arguments: dict) -> list[TextContent]:
    if name == "query_database":
        pool = await get_db_pool()  # Lazy init on first call
        query = arguments["query"]
        params = arguments.get("params", [])

        # Security: Only allow SELECT queries
        if not query.strip().upper().startswith("SELECT"):
            return [TextContent(
                type="text",
                text=json.dumps({"error": "Only SELECT queries allowed"})
            )]

        rows = await pool.fetch(query, *params)
        return [TextContent(
            type="text",
            text=json.dumps([dict(row) for row in rows], default=str)
        )]

    return [TextContent(type="text", text=json.dumps({"error": "Unknown tool"}))]
```

#### Pattern 3: Multi-Agent Pipeline with Quality Gates

**Use Case:** Complex data processing with validation at each stage

```python
from dataclasses import dataclass
from enum import Enum
from typing import Any, Optional
import asyncio

class GateStatus(Enum):
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"

class GateSeverity(Enum):
    BLOCKING = "blocking"
    WARNING = "warning"
    INFO = "info"

@dataclass
class QualityGate:
    """Result of a quality gate check."""
    name: str
    status: GateStatus
    severity: GateSeverity
    message: str
    details: Optional[dict] = None

@dataclass
class AgentResult:
    """Result from an agent in the pipeline."""
    agent_name: str
    success: bool
    data: Any
    gates: list[QualityGate]
    duration_ms: float
    error: Optional[str] = None

class PipelineAgent:
    """Base class for pipeline agents."""

    def __init__(self, name: str):
        self.name = name

    async def process(self, input_data: Any) -> AgentResult:
        """Process input and return result with quality gates."""
        raise NotImplementedError

    def create_gate(
        self,
        name: str,
        passed: bool,
        severity: GateSeverity,
        message: str,
        details: dict = None
    ) -> QualityGate:
        """Create a quality gate result."""
        return QualityGate(
            name=name,
            status=GateStatus.PASSED if passed else GateStatus.FAILED,
            severity=severity,
            message=message,
            details=details
        )

class ValidationAgent(PipelineAgent):
    """Validates input data."""

    def __init__(self):
        super().__init__("validator")

    async def process(self, input_data: dict) -> AgentResult:
        import time
        start = time.time()
        gates = []

        # Gate 1: Required fields
        required = ["query", "user_id"]
        missing = [f for f in required if f not in input_data]
        gates.append(self.create_gate(
            "required_fields",
            len(missing) == 0,
            GateSeverity.BLOCKING,
            f"Missing fields: {missing}" if missing else "All required fields present"
        ))

        # Gate 2: Query length
        query = input_data.get("query", "")
        gates.append(self.create_gate(
            "query_length",
            0 < len(query) < 10000,
            GateSeverity.BLOCKING,
            f"Query length: {len(query)} chars",
            {"length": len(query), "max": 10000}
        ))

        # Gate 3: No dangerous patterns
        dangerous_patterns = ["DROP", "DELETE", "TRUNCATE"]
        found = [p for p in dangerous_patterns if p in query.upper()]
        gates.append(self.create_gate(
            "safe_query",
            len(found) == 0,
            GateSeverity.BLOCKING,
            f"Dangerous patterns found: {found}" if found else "No dangerous patterns"
        ))

        # Check for blocking failures
        blocking_failures = [g for g in gates if g.status == GateStatus.FAILED and g.severity == GateSeverity.BLOCKING]

        return AgentResult(
            agent_name=self.name,
            success=len(blocking_failures) == 0,
            data=input_data if not blocking_failures else None,
            gates=gates,
            duration_ms=(time.time() - start) * 1000
        )

class EnrichmentAgent(PipelineAgent):
    """Enriches data with additional context."""

    def __init__(self):
        super().__init__("enricher")

    async def process(self, input_data: dict) -> AgentResult:
        import time
        start = time.time()
        gates = []

        # Add metadata
        enriched = {
            **input_data,
            "processed_at": datetime.utcnow().isoformat(),
            "version": "1.0"
        }

        gates.append(self.create_gate(
            "enrichment_complete",
            True,
            GateSeverity.INFO,
            "Data enriched successfully"
        ))

        return AgentResult(
            agent_name=self.name,
            success=True,
            data=enriched,
            gates=gates,
            duration_ms=(time.time() - start) * 1000
        )

class PipelineOrchestrator:
    """Orchestrates multi-agent pipeline."""

    def __init__(self, agents: list[PipelineAgent]):
        self.agents = agents
        self.results: list[AgentResult] = []

    async def run(self, input_data: Any) -> dict:
        """Run pipeline with all agents sequentially."""
        self.results = []
        current_data = input_data

        for agent in self.agents:
            result = await agent.process(current_data)
            self.results.append(result)

            # Stop on blocking failure
            if not result.success:
                return {
                    "success": False,
                    "stopped_at": agent.name,
                    "results": [self._serialize_result(r) for r in self.results],
                    "output": None
                }

            current_data = result.data

        return {
            "success": True,
            "results": [self._serialize_result(r) for r in self.results],
            "output": current_data
        }

    def _serialize_result(self, result: AgentResult) -> dict:
        return {
            "agent": result.agent_name,
            "success": result.success,
            "duration_ms": result.duration_ms,
            "gates": [
                {
                    "name": g.name,
                    "status": g.status.value,
                    "severity": g.severity.value,
                    "message": g.message
                }
                for g in result.gates
            ],
            "error": result.error
        }

# MCP Server with Pipeline
server = Server("pipeline-mcp")

@server.list_tools()
async def list_tools() -> list[Tool]:
    return [
        Tool(
            name="process_query",
            description="Process a query through validation and enrichment pipeline",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "user_id": {"type": "string"}
                },
                "required": ["query", "user_id"]
            }
        )
    ]

@server.call_tool()
async def call_tool(name: str, arguments: dict) -> list[TextContent]:
    if name == "process_query":
        pipeline = PipelineOrchestrator([
            ValidationAgent(),
            EnrichmentAgent()
        ])

        result = await pipeline.run(arguments)

        return [TextContent(
            type="text",
            text=json.dumps(result, indent=2, default=str)
        )]

    return [TextContent(type="text", text=json.dumps({"error": "Unknown tool"}))]
```

#### Pattern 4: SSE Transport for Remote Deployment

**Use Case:** Web-deployed MCP server

```python
from mcp.server import Server
from mcp.server.sse import SseServerTransport
from starlette.applications import Starlette
from starlette.routing import Route
from starlette.responses import Response
import uvicorn

server = Server("remote-mcp")

# Tool definitions...
@server.list_tools()
async def list_tools() -> list[Tool]:
    return [
        Tool(
            name="echo",
            description="Echo back a message",
            inputSchema={
                "type": "object",
                "properties": {"message": {"type": "string"}},
                "required": ["message"]
            }
        )
    ]

@server.call_tool()
async def call_tool(name: str, arguments: dict) -> list[TextContent]:
    if name == "echo":
        return [TextContent(type="text", text=arguments["message"])]
    return [TextContent(type="text", text="Unknown tool")]

# SSE endpoint handler
sse = SseServerTransport("/messages")

async def handle_sse(request):
    """Handle SSE connection."""
    async with sse.connect_sse(
        request.scope, request.receive, request._send
    ) as streams:
        await server.run(
            streams[0],
            streams[1],
            server.create_initialization_options()
        )
    return Response()

async def handle_messages(request):
    """Handle message POST."""
    await sse.handle_post_message(request.scope, request.receive, request._send)
    return Response()

# Create Starlette app
app = Starlette(
    routes=[
        Route("/sse", endpoint=handle_sse),
        Route("/messages", endpoint=handle_messages, methods=["POST"]),
    ]
)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8080)
```

### 3. Best Practices

1. **Use Lazy Initialization** - Don't create connections at import time
   ```python
   # Bad: Connection at import
   pool = asyncpg.create_pool(...)

   # Good: Lazy initialization
   _pool = None
   async def get_pool():
       global _pool
       if _pool is None:
           _pool = await asyncpg.create_pool(...)
       return _pool
   ```

2. **Return JSON from All Tools** - Use `json.dumps` with `default=str` for serialization
   ```python
   return [TextContent(
       type="text",
       text=json.dumps(result, indent=2, default=str)
   )]
   ```

3. **Implement Graceful Error Handling** - Never let exceptions crash the server
   ```python
   @server.call_tool()
   async def call_tool(name: str, arguments: dict):
       try:
           result = await process(name, arguments)
           return [TextContent(type="text", text=json.dumps(result))]
       except Exception as e:
           return [TextContent(type="text", text=json.dumps({
               "error": str(e),
               "tool": name
           }))]
   ```

4. **Use Type Hints Throughout** - Better IDE support and documentation

5. **Environment-Driven Configuration** - No hardcoded values
   ```python
   import os
   DB_HOST = os.environ.get("DB_HOST", "localhost")
   ```

## ðŸ”§ Common Tasks

### Task 1: Create Tool with Complex Schema

**Goal:** Define a tool with nested objects and arrays

```python
Tool(
    name="search_documents",
    description="""Search documents with filters and sorting.

Supports:
- Full-text search across title and content
- Date range filtering
- Category filtering
- Pagination and sorting
""",
    inputSchema={
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "Search query (supports wildcards)"
            },
            "filters": {
                "type": "object",
                "description": "Optional filters",
                "properties": {
                    "categories": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "Filter by categories"
                    },
                    "date_range": {
                        "type": "object",
                        "properties": {
                            "start": {
                                "type": "string",
                                "format": "date",
                                "description": "Start date (YYYY-MM-DD)"
                            },
                            "end": {
                                "type": "string",
                                "format": "date",
                                "description": "End date (YYYY-MM-DD)"
                            }
                        }
                    },
                    "status": {
                        "type": "string",
                        "enum": ["draft", "published", "archived"]
                    }
                }
            },
            "pagination": {
                "type": "object",
                "properties": {
                    "page": {"type": "integer", "minimum": 1, "default": 1},
                    "page_size": {"type": "integer", "minimum": 1, "maximum": 100, "default": 20}
                }
            },
            "sort": {
                "type": "object",
                "properties": {
                    "field": {
                        "type": "string",
                        "enum": ["relevance", "date", "title"]
                    },
                    "order": {
                        "type": "string",
                        "enum": ["asc", "desc"],
                        "default": "desc"
                    }
                }
            }
        },
        "required": ["query"]
    }
)
```

### Task 2: Implement Resources

**Goal:** Expose data as MCP resources

```python
from mcp.types import Resource, ResourceTemplate

@server.list_resources()
async def list_resources() -> list[Resource]:
    """List available resources."""
    return [
        Resource(
            uri="config://app/settings",
            name="Application Settings",
            description="Current application configuration",
            mimeType="application/json"
        ),
        Resource(
            uri="data://customers/count",
            name="Customer Count",
            description="Total number of customers",
            mimeType="text/plain"
        )
    ]

@server.list_resource_templates()
async def list_resource_templates() -> list[ResourceTemplate]:
    """List resource templates for dynamic resources."""
    return [
        ResourceTemplate(
            uriTemplate="data://customers/{customer_id}",
            name="Customer Details",
            description="Get details for a specific customer",
            mimeType="application/json"
        ),
        ResourceTemplate(
            uriTemplate="data://orders/{order_id}",
            name="Order Details",
            description="Get details for a specific order",
            mimeType="application/json"
        )
    ]

@server.read_resource()
async def read_resource(uri: str) -> str:
    """Read a resource by URI."""
    if uri == "config://app/settings":
        settings = {
            "version": "1.0.0",
            "environment": os.environ.get("ENV", "development"),
            "features": {
                "caching": True,
                "logging": True
            }
        }
        return json.dumps(settings, indent=2)

    if uri == "data://customers/count":
        pool = await get_db_pool()
        row = await pool.fetchrow("SELECT COUNT(*) FROM customers")
        return str(row["count"])

    # Handle templated resources
    if uri.startswith("data://customers/"):
        customer_id = uri.split("/")[-1]
        pool = await get_db_pool()
        row = await pool.fetchrow(
            "SELECT * FROM customers WHERE id = $1",
            customer_id
        )
        if row:
            return json.dumps(dict(row), default=str)
        return json.dumps({"error": "Customer not found"})

    return json.dumps({"error": f"Unknown resource: {uri}"})
```

### Task 3: Implement Prompt Templates

**Goal:** Provide reusable prompt templates

```python
from mcp.types import Prompt, PromptArgument, PromptMessage, TextContent

@server.list_prompts()
async def list_prompts() -> list[Prompt]:
    """List available prompt templates."""
    return [
        Prompt(
            name="analyze_data",
            description="Analyze data from a specific table",
            arguments=[
                PromptArgument(
                    name="table_name",
                    description="Name of the table to analyze",
                    required=True
                ),
                PromptArgument(
                    name="focus_area",
                    description="Specific aspect to focus on (e.g., trends, outliers)",
                    required=False
                )
            ]
        ),
        Prompt(
            name="generate_report",
            description="Generate a report for a date range",
            arguments=[
                PromptArgument(
                    name="report_type",
                    description="Type of report (sales, inventory, users)",
                    required=True
                ),
                PromptArgument(
                    name="start_date",
                    description="Report start date (YYYY-MM-DD)",
                    required=True
                ),
                PromptArgument(
                    name="end_date",
                    description="Report end date (YYYY-MM-DD)",
                    required=True
                )
            ]
        )
    ]

@server.get_prompt()
async def get_prompt(name: str, arguments: dict | None) -> list[PromptMessage]:
    """Get a prompt template with arguments filled in."""
    args = arguments or {}

    if name == "analyze_data":
        table = args.get("table_name", "unknown")
        focus = args.get("focus_area", "general patterns")

        return [
            PromptMessage(
                role="user",
                content=TextContent(
                    type="text",
                    text=f"""Please analyze the data in the '{table}' table.

Focus on: {focus}

Use the available tools to:
1. First, understand the table structure
2. Query relevant data
3. Identify key insights
4. Provide recommendations

Be thorough but concise in your analysis."""
                )
            )
        ]

    if name == "generate_report":
        report_type = args.get("report_type", "general")
        start = args.get("start_date", "unknown")
        end = args.get("end_date", "unknown")

        return [
            PromptMessage(
                role="user",
                content=TextContent(
                    type="text",
                    text=f"""Generate a {report_type} report for the period {start} to {end}.

Include:
- Executive summary
- Key metrics
- Trends and changes
- Recommendations

Format the report professionally with clear sections."""
                )
            )
        ]

    return []
```

### Task 4: Batch Processing with Concurrency Control

**Goal:** Process multiple items efficiently

```python
import asyncio
from dataclasses import dataclass
from typing import TypeVar, Callable, Awaitable

T = TypeVar("T")
R = TypeVar("R")

@dataclass
class BatchResult:
    """Result of batch processing."""
    total: int
    successful: int
    failed: int
    results: list[dict]
    duration_ms: float

class BatchProcessor:
    """Process items in batches with concurrency control."""

    def __init__(self, max_concurrent: int = 5):
        self.max_concurrent = max_concurrent

    async def process(
        self,
        items: list[T],
        handler: Callable[[T, int], Awaitable[R]]
    ) -> BatchResult:
        """Process items with controlled concurrency."""
        import time
        start = time.time()

        semaphore = asyncio.Semaphore(self.max_concurrent)
        results = []
        successful = 0
        failed = 0

        async def process_item(item: T, index: int) -> dict:
            nonlocal successful, failed
            async with semaphore:
                try:
                    result = await handler(item, index)
                    successful += 1
                    return {"index": index, "success": True, "result": result}
                except Exception as e:
                    failed += 1
                    return {"index": index, "success": False, "error": str(e)}

        tasks = [process_item(item, i) for i, item in enumerate(items)]
        results = await asyncio.gather(*tasks)

        return BatchResult(
            total=len(items),
            successful=successful,
            failed=failed,
            results=sorted(results, key=lambda x: x["index"]),
            duration_ms=(time.time() - start) * 1000
        )

# MCP Tool using batch processor
@server.call_tool()
async def call_tool(name: str, arguments: dict) -> list[TextContent]:
    if name == "batch_process":
        items = arguments.get("items", [])

        async def process_single(item: dict, index: int) -> dict:
            # Simulate processing
            await asyncio.sleep(0.1)
            return {"processed": item, "index": index}

        processor = BatchProcessor(max_concurrent=3)
        result = await processor.process(items, process_single)

        return [TextContent(
            type="text",
            text=json.dumps({
                "total": result.total,
                "successful": result.successful,
                "failed": result.failed,
                "duration_ms": result.duration_ms
            })
        )]
```

## âš™ï¸ Configuration

### MCP Server Configuration (.mcp.json)

```json
{
  "mcpServers": {
    "my-server": {
      "command": "python",
      "args": ["path/to/server.py"],
      "env": {
        "DB_HOST": "localhost",
        "DB_PORT": "5432",
        "DB_NAME": "mydb",
        "DB_USER": "app_user",
        "DB_PASSWORD": "${DB_PASSWORD}",
        "REDIS_URL": "redis://localhost:6379",
        "LOG_LEVEL": "INFO"
      }
    }
  }
}
```

### Claude Desktop Configuration

```json
{
  "mcpServers": {
    "database-tools": {
      "command": "python",
      "args": ["/absolute/path/to/server.py"],
      "env": {
        "DB_CONNECTION_STRING": "postgresql://user:pass@localhost/db"
      }
    },
    "api-tools": {
      "command": "node",
      "args": ["/absolute/path/to/server.js"]
    }
  }
}
```

### Dockerfile for MCP Server

```dockerfile
FROM python:3.11-slim-bookworm

# Security: Non-root user
RUN groupadd -g 10001 mcpuser && \
    useradd -u 10001 -g mcpuser -s /bin/bash -m mcpuser

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY --chown=mcpuser:mcpuser . .

# Switch to non-root user
USER mcpuser

# Environment variables
ENV PYTHONUNBUFFERED=1
ENV LOG_LEVEL=INFO

# Health check
HEALTHCHECK --interval=30s --timeout=5s --start-period=5s --retries=3 \
    CMD python -c "import sys; sys.exit(0)"

# Run server
CMD ["python", "server.py"]
```

### Requirements (requirements.txt)

```
mcp>=1.0.0
httpx>=0.25.0
asyncpg>=0.29.0
redis>=5.0.0
pydantic>=2.0.0
python-dotenv>=1.0.0
structlog>=23.0.0
```

## ðŸ› Troubleshooting

### Issue 1: Tool Not Appearing in Client

**Symptoms:**
- Tool defined but not listed in Claude
- "Unknown tool" errors

**Causes:**
- Server not running
- Incorrect path in config
- Tool registration error

**Solution:**
```python
# 1. Verify server is running
# Check for startup errors in console

# 2. Test tool registration
@server.list_tools()
async def list_tools() -> list[Tool]:
    tools = [
        Tool(name="test", description="Test", inputSchema={"type": "object"})
    ]
    print(f"Registering {len(tools)} tools")  # Debug
    return tools

# 3. Check .mcp.json path is absolute
{
  "mcpServers": {
    "my-server": {
      "command": "python",
      "args": ["/Users/me/projects/mcp-server/server.py"]  # Absolute path!
    }
  }
}
```

### Issue 2: Connection Timeout

**Symptoms:**
- "Connection refused" errors
- Server starts but client can't connect

**Causes:**
- stdio not properly configured
- Server exiting early

**Solution:**
```python
import asyncio
import sys

async def main():
    # Ensure we're using stdio properly
    from mcp.server.stdio import stdio_server

    try:
        async with stdio_server() as (read_stream, write_stream):
            print("Server starting...", file=sys.stderr)  # Debug to stderr
            await server.run(
                read_stream,
                write_stream,
                server.create_initialization_options()
            )
    except Exception as e:
        print(f"Server error: {e}", file=sys.stderr)
        raise

if __name__ == "__main__":
    asyncio.run(main())
```

### Issue 3: JSON Serialization Errors

**Symptoms:**
- "Object not JSON serializable" errors
- Tool returns error instead of data

**Causes:**
- datetime objects
- Decimal types
- Custom classes

**Solution:**
```python
import json
from datetime import datetime, date
from decimal import Decimal

def json_serializer(obj):
    """Custom JSON serializer for non-standard types."""
    if isinstance(obj, (datetime, date)):
        return obj.isoformat()
    if isinstance(obj, Decimal):
        return float(obj)
    if hasattr(obj, "__dict__"):
        return obj.__dict__
    raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

# Usage in tool handler
return [TextContent(
    type="text",
    text=json.dumps(result, default=json_serializer, indent=2)
)]
```

### Issue 4: Memory Leaks

**Symptoms:**
- Server memory grows over time
- Eventually crashes or slows down

**Causes:**
- Connections not closed
- Large results cached

**Solution:**
```python
import weakref
from contextlib import asynccontextmanager

class ConnectionManager:
    """Manage connections with automatic cleanup."""

    def __init__(self):
        self._connections: weakref.WeakSet = weakref.WeakSet()

    @asynccontextmanager
    async def get_connection(self):
        """Get a connection with automatic cleanup."""
        conn = await create_connection()
        self._connections.add(conn)
        try:
            yield conn
        finally:
            await conn.close()

    async def cleanup_all(self):
        """Close all tracked connections."""
        for conn in list(self._connections):
            try:
                await conn.close()
            except Exception:
                pass
```

## ðŸš€ Performance Optimization

### Optimization 1: Connection Pooling

```python
class PooledDatabaseClient:
    """Database client with connection pooling."""

    def __init__(self, dsn: str, min_size: int = 2, max_size: int = 10):
        self.dsn = dsn
        self.min_size = min_size
        self.max_size = max_size
        self._pool = None

    async def _get_pool(self):
        if self._pool is None:
            import asyncpg
            self._pool = await asyncpg.create_pool(
                self.dsn,
                min_size=self.min_size,
                max_size=self.max_size,
                max_inactive_connection_lifetime=300
            )
        return self._pool

    async def execute(self, query: str, *args):
        pool = await self._get_pool()
        async with pool.acquire() as conn:
            return await conn.fetch(query, *args)

    async def close(self):
        if self._pool:
            await self._pool.close()
            self._pool = None
```

### Optimization 2: Result Caching

```python
from functools import wraps
import hashlib
import time

class LRUCache:
    """Simple LRU cache with TTL."""

    def __init__(self, max_size: int = 100, ttl: int = 300):
        self.max_size = max_size
        self.ttl = ttl
        self.cache: dict[str, tuple[float, Any]] = {}

    def get(self, key: str) -> Any | None:
        if key in self.cache:
            timestamp, value = self.cache[key]
            if time.time() - timestamp < self.ttl:
                return value
            del self.cache[key]
        return None

    def set(self, key: str, value: Any):
        # Evict oldest if at capacity
        if len(self.cache) >= self.max_size:
            oldest = min(self.cache.items(), key=lambda x: x[1][0])
            del self.cache[oldest[0]]
        self.cache[key] = (time.time(), value)

def cached(cache: LRUCache):
    """Decorator for caching async function results."""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            key = hashlib.md5(
                f"{func.__name__}:{args}:{kwargs}".encode()
            ).hexdigest()

            cached_result = cache.get(key)
            if cached_result is not None:
                return cached_result

            result = await func(*args, **kwargs)
            cache.set(key, result)
            return result
        return wrapper
    return decorator

# Usage
cache = LRUCache(max_size=100, ttl=60)

@cached(cache)
async def expensive_query(query: str) -> list[dict]:
    pool = await get_db_pool()
    rows = await pool.fetch(query)
    return [dict(row) for row in rows]
```

### Optimization 3: Streaming Large Results

```python
async def stream_large_result(query: str, chunk_size: int = 1000):
    """Stream large query results in chunks."""
    pool = await get_db_pool()

    async with pool.acquire() as conn:
        async with conn.transaction():
            # Use cursor for streaming
            cursor = await conn.cursor(query)

            while True:
                rows = await cursor.fetch(chunk_size)
                if not rows:
                    break
                yield [dict(row) for row in rows]

# Usage in tool handler
async def handle_large_query(query: str) -> list[TextContent]:
    chunks = []
    async for chunk in stream_large_result(query):
        chunks.extend(chunk)

        # Return early if too large
        if len(chunks) > 10000:
            return [TextContent(
                type="text",
                text=json.dumps({
                    "data": chunks[:10000],
                    "truncated": True,
                    "message": "Result truncated to 10,000 rows"
                })
            )]

    return [TextContent(type="text", text=json.dumps({"data": chunks}))]
```

## ðŸ”’ Security Best Practices

### 1. Input Validation

```python
from pydantic import BaseModel, validator, Field
from typing import Optional

class QueryInput(BaseModel):
    """Validated query input."""
    query: str = Field(..., min_length=1, max_length=10000)
    params: Optional[list[str]] = Field(default_factory=list, max_items=50)

    @validator("query")
    def validate_query(cls, v):
        # Only allow SELECT queries
        if not v.strip().upper().startswith("SELECT"):
            raise ValueError("Only SELECT queries are allowed")

        # Block dangerous patterns
        dangerous = ["DROP", "DELETE", "TRUNCATE", "UPDATE", "INSERT", "ALTER"]
        upper = v.upper()
        for pattern in dangerous:
            if pattern in upper:
                raise ValueError(f"Dangerous pattern detected: {pattern}")

        return v

@server.call_tool()
async def call_tool(name: str, arguments: dict) -> list[TextContent]:
    if name == "query":
        try:
            # Validate input
            validated = QueryInput(**arguments)
            result = await execute_query(validated.query, validated.params)
            return [TextContent(type="text", text=json.dumps(result))]
        except ValueError as e:
            return [TextContent(type="text", text=json.dumps({"error": str(e)}))]
```

### 2. Credential Management

```python
import os
from typing import Protocol

class SecretProvider(Protocol):
    def get(self, key: str) -> str: ...

class EnvironmentSecrets:
    """Get secrets from environment variables."""
    def get(self, key: str) -> str:
        value = os.environ.get(key)
        if not value:
            raise ValueError(f"Secret {key} not found")
        return value

class AWSSecretsManager:
    """Get secrets from AWS Secrets Manager."""
    def __init__(self, region: str = "us-east-1"):
        import boto3
        self.client = boto3.client("secretsmanager", region_name=region)

    def get(self, key: str) -> str:
        response = self.client.get_secret_value(SecretId=key)
        return response["SecretString"]

# Use dependency injection
def create_db_client(secrets: SecretProvider):
    return DatabaseClient(
        host=secrets.get("DB_HOST"),
        password=secrets.get("DB_PASSWORD")
    )
```

### 3. Rate Limiting

```python
import time
from collections import defaultdict

class RateLimiter:
    """Per-user rate limiting."""

    def __init__(self, max_requests: int = 100, window: int = 60):
        self.max_requests = max_requests
        self.window = window
        self.requests: dict[str, list[float]] = defaultdict(list)

    def is_allowed(self, user_id: str) -> bool:
        """Check if request is allowed."""
        now = time.time()
        window_start = now - self.window

        # Clean old requests
        self.requests[user_id] = [
            t for t in self.requests[user_id]
            if t > window_start
        ]

        # Check limit
        if len(self.requests[user_id]) >= self.max_requests:
            return False

        # Record request
        self.requests[user_id].append(now)
        return True

rate_limiter = RateLimiter(max_requests=100, window=60)

@server.call_tool()
async def call_tool(name: str, arguments: dict) -> list[TextContent]:
    user_id = arguments.get("_user_id", "anonymous")

    if not rate_limiter.is_allowed(user_id):
        return [TextContent(
            type="text",
            text=json.dumps({"error": "Rate limit exceeded"})
        )]

    # Process request...
```

## ðŸ§ª Testing Strategies

### Unit Testing

```python
import pytest
from unittest.mock import AsyncMock, patch

@pytest.fixture
def server():
    """Create test server instance."""
    return Server("test-server")

class TestToolHandler:
    """Unit tests for tool handlers."""

    @pytest.mark.asyncio
    async def test_greet_returns_greeting(self):
        """Test greet tool returns proper greeting."""
        result = await call_tool("greet", {"name": "Alice"})

        assert len(result) == 1
        assert "Hello, Alice" in result[0].text

    @pytest.mark.asyncio
    async def test_calculate_add(self):
        """Test calculate tool performs addition."""
        result = await call_tool("calculate", {
            "operation": "add",
            "a": 5,
            "b": 3
        })

        data = json.loads(result[0].text)
        assert data == 8

    @pytest.mark.asyncio
    async def test_unknown_tool_returns_error(self):
        """Test unknown tool returns error."""
        result = await call_tool("nonexistent", {})

        data = json.loads(result[0].text)
        assert "error" in data

class TestDatabaseTool:
    """Tests for database tool with mocking."""

    @pytest.mark.asyncio
    async def test_query_executes_select(self):
        """Test query tool executes SELECT."""
        mock_pool = AsyncMock()
        mock_pool.fetch.return_value = [
            {"id": 1, "name": "Test"}
        ]

        with patch("server.get_db_pool", return_value=mock_pool):
            result = await call_tool("query_database", {
                "query": "SELECT * FROM users"
            })

        data = json.loads(result[0].text)
        assert len(data) == 1
        assert data[0]["name"] == "Test"

    @pytest.mark.asyncio
    async def test_query_rejects_non_select(self):
        """Test query tool rejects non-SELECT queries."""
        result = await call_tool("query_database", {
            "query": "DELETE FROM users"
        })

        data = json.loads(result[0].text)
        assert "error" in data
        assert "SELECT" in data["error"]
```

### Integration Testing

```python
@pytest.mark.integration
class TestMCPServer:
    """Integration tests for MCP server."""

    @pytest.fixture
    async def client(self):
        """Create MCP client for testing."""
        from mcp.client import Client
        from mcp.client.stdio import stdio_client

        async with stdio_client(
            command="python",
            args=["server.py"]
        ) as (read, write):
            client = Client("test-client")
            await client.connect(read, write)
            yield client

    @pytest.mark.asyncio
    async def test_list_tools(self, client):
        """Test listing tools via MCP protocol."""
        tools = await client.list_tools()

        assert len(tools) > 0
        tool_names = [t.name for t in tools]
        assert "greet" in tool_names

    @pytest.mark.asyncio
    async def test_call_tool(self, client):
        """Test calling tool via MCP protocol."""
        result = await client.call_tool("greet", {"name": "Test"})

        assert "Hello, Test" in result
```

## ðŸ“Š Monitoring & Observability

### Metrics with Prometheus

```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import time

# Metrics
tool_calls_total = Counter(
    "mcp_tool_calls_total",
    "Total tool calls",
    ["tool_name", "status"]
)

tool_latency = Histogram(
    "mcp_tool_latency_seconds",
    "Tool call latency",
    ["tool_name"],
    buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
)

active_connections = Gauge(
    "mcp_active_connections",
    "Number of active MCP connections"
)

class InstrumentedServer:
    """MCP server with metrics."""

    def __init__(self, server: Server):
        self.server = server
        self._wrap_call_tool()

    def _wrap_call_tool(self):
        original = self.server.call_tool

        async def instrumented_call_tool(name: str, arguments: dict):
            start = time.time()
            try:
                result = await original(name, arguments)
                tool_calls_total.labels(tool_name=name, status="success").inc()
                return result
            except Exception as e:
                tool_calls_total.labels(tool_name=name, status="error").inc()
                raise
            finally:
                tool_latency.labels(tool_name=name).observe(time.time() - start)

        self.server.call_tool = instrumented_call_tool
```

### Structured Logging

```python
import structlog

logger = structlog.get_logger()

def configure_logging():
    """Configure structured logging."""
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.JSONRenderer()
        ],
        wrapper_class=structlog.stdlib.BoundLogger,
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
    )

@server.call_tool()
async def call_tool(name: str, arguments: dict) -> list[TextContent]:
    log = logger.bind(tool=name, args_keys=list(arguments.keys()))

    log.info("tool_call_start")
    start = time.time()

    try:
        result = await handle_tool(name, arguments)
        log.info("tool_call_success", duration_ms=(time.time() - start) * 1000)
        return result
    except Exception as e:
        log.error("tool_call_error", error=str(e), duration_ms=(time.time() - start) * 1000)
        raise
```

## ðŸ’¡ Pro Tips

1. **Use lazy initialization** - Defer expensive connections until first use

2. **Always return JSON** - Use `json.dumps(result, default=str)` for consistent output

3. **Implement graceful degradation** - Return helpful errors, not stack traces

4. **Use async/await everywhere** - MCP is async-first; blocking calls hurt performance

5. **Log tool call metrics** - Track latency and error rates for optimization

6. **Validate inputs early** - Fail fast with clear error messages

7. **Use connection pooling** - Never create connections per-request

8. **Implement health checks** - Expose server health for monitoring

9. **Follow the pipeline pattern** - Sequential agents with quality gates

10. **Use type hints** - Better IDE support and self-documentation

## ðŸš¨ Common Mistakes to Avoid

1. âŒ **Synchronous database calls** - Always use async drivers (asyncpg, motor)

2. âŒ **Missing error handling** - Every tool call needs try/except

3. âŒ **Hardcoded credentials** - Use environment variables or secret managers

4. âŒ **No connection pooling** - Leads to pool exhaustion under load

5. âŒ **Returning non-JSON** - TextContent expects string content

6. âŒ **Missing input validation** - SQL injection and other attacks

7. âŒ **Blocking operations** - Use `run_in_executor` for CPU-bound work

8. âŒ **No timeout handling** - Queries can hang forever without limits

9. âŒ **Ignoring transactions** - Data consistency issues

10. âŒ **Not closing connections** - Memory leaks and resource exhaustion

11. âŒ **Missing health checks** - Silent failures in production

12. âŒ **Over-complicated schemas** - Keep tool inputs simple

13. âŒ **No logging** - Debugging becomes impossible

14. âŒ **Skipping tests** - Quality and reliability suffer

15. âŒ **Ignoring MCP versions** - Protocol compatibility issues

## ðŸ“‹ Production Checklist

### Server Configuration
- [ ] Environment variables for all credentials
- [ ] Non-root user in container
- [ ] Health check endpoint implemented
- [ ] Graceful shutdown handling
- [ ] Connection pooling configured

### Security
- [ ] Input validation on all tools
- [ ] SQL injection prevention (parameterized queries)
- [ ] Credentials not logged
- [ ] HTTPS/TLS for SSE transport
- [ ] Rate limiting implemented

### Reliability
- [ ] Error handling for all tools
- [ ] Timeout limits configured
- [ ] Retry logic for transient failures
- [ ] Circuit breaker for external calls

### Monitoring
- [ ] Tool call metrics tracked
- [ ] Error rates monitored
- [ ] Latency tracking enabled
- [ ] Log aggregation configured
- [ ] Alerting on failures

### Testing
- [ ] Unit tests for all tools
- [ ] Integration tests passing
- [ ] Load testing completed
- [ ] Error scenarios tested

### Documentation
- [ ] Tool descriptions complete
- [ ] Input schemas documented
- [ ] Examples provided
- [ ] Changelog updated

---

**Agent Version:** 1.0
**Last Updated:** 2025-12-17
**Expertise Level:** Expert
**Focus:** MCP Server Architecture, Tool Design, Quality Gates, Multi-Agent Pipelines
