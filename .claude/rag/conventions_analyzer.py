"""
Coding Conventions Analyzer
Automatically detects coding patterns and conventions from existing codebase.

Analyzes:
- Naming conventions (camelCase, snake_case, PascalCase)
- File organization patterns
- Import styles
- Documentation patterns
- Error handling patterns
- Testing patterns
- Framework-specific conventions
"""

import os
import re
from typing import List, Dict, Any, Optional, Set, Tuple
from dataclasses import dataclass, field
from pathlib import Path
from collections import Counter
from enum import Enum
import json


class NamingStyle(Enum):
    """Naming convention styles"""
    CAMEL_CASE = "camelCase"
    PASCAL_CASE = "PascalCase"
    SNAKE_CASE = "snake_case"
    KEBAB_CASE = "kebab-case"
    SCREAMING_SNAKE = "SCREAMING_SNAKE_CASE"
    UNKNOWN = "unknown"


@dataclass
class ConventionRule:
    """A detected convention rule"""
    category: str
    rule: str
    confidence: float  # 0.0 to 1.0
    examples: List[str] = field(default_factory=list)
    source_files: List[str] = field(default_factory=list)


@dataclass
class ProjectConventions:
    """Complete project conventions"""
    project_type: str
    language: str
    frameworks: List[str]
    rules: List[ConventionRule]

    # Detected patterns
    naming_style_variables: NamingStyle = NamingStyle.UNKNOWN
    naming_style_functions: NamingStyle = NamingStyle.UNKNOWN
    naming_style_classes: NamingStyle = NamingStyle.UNKNOWN
    naming_style_constants: NamingStyle = NamingStyle.UNKNOWN
    naming_style_files: NamingStyle = NamingStyle.UNKNOWN

    # File organization
    source_directories: List[str] = field(default_factory=list)
    test_directories: List[str] = field(default_factory=list)
    config_files: List[str] = field(default_factory=list)

    # Documentation
    has_docstrings: bool = False
    docstring_style: str = ""  # google, numpy, sphinx, jsdoc
    has_type_hints: bool = False

    # Error handling
    error_handling_pattern: str = ""

    # Testing
    test_framework: str = ""
    test_naming_pattern: str = ""

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            "project_type": self.project_type,
            "language": self.language,
            "frameworks": self.frameworks,
            "naming": {
                "variables": self.naming_style_variables.value,
                "functions": self.naming_style_functions.value,
                "classes": self.naming_style_classes.value,
                "constants": self.naming_style_constants.value,
                "files": self.naming_style_files.value,
            },
            "file_organization": {
                "source_directories": self.source_directories,
                "test_directories": self.test_directories,
                "config_files": self.config_files,
            },
            "documentation": {
                "has_docstrings": self.has_docstrings,
                "docstring_style": self.docstring_style,
                "has_type_hints": self.has_type_hints,
            },
            "error_handling": self.error_handling_pattern,
            "testing": {
                "framework": self.test_framework,
                "naming_pattern": self.test_naming_pattern,
            },
            "rules": [
                {
                    "category": r.category,
                    "rule": r.rule,
                    "confidence": r.confidence,
                    "examples": r.examples[:3],
                }
                for r in self.rules
            ],
        }

    def to_markdown(self) -> str:
        """Generate markdown conventions file"""
        md = []
        md.append("# Project Coding Conventions")
        md.append("")
        md.append("*Auto-generated by Adaptive RAG System*")
        md.append("")
        md.append(f"**Project Type**: {self.project_type}")
        md.append(f"**Primary Language**: {self.language}")
        if self.frameworks:
            md.append(f"**Frameworks**: {', '.join(self.frameworks)}")
        md.append("")

        # Naming conventions
        md.append("## Naming Conventions")
        md.append("")
        md.append("| Element | Style | Example |")
        md.append("|---------|-------|---------|")
        md.append(f"| Variables | `{self.naming_style_variables.value}` | `userName`, `user_name` |")
        md.append(f"| Functions | `{self.naming_style_functions.value}` | `getUserById`, `get_user_by_id` |")
        md.append(f"| Classes | `{self.naming_style_classes.value}` | `UserService`, `user_service` |")
        md.append(f"| Constants | `{self.naming_style_constants.value}` | `MAX_RETRIES`, `maxRetries` |")
        md.append(f"| Files | `{self.naming_style_files.value}` | `user-service.ts`, `user_service.py` |")
        md.append("")

        # File organization
        md.append("## File Organization")
        md.append("")
        if self.source_directories:
            md.append(f"**Source Directories**: `{', '.join(self.source_directories)}`")
        if self.test_directories:
            md.append(f"**Test Directories**: `{', '.join(self.test_directories)}`")
        if self.config_files:
            md.append(f"**Config Files**: `{', '.join(self.config_files[:5])}`")
        md.append("")

        # Documentation
        md.append("## Documentation")
        md.append("")
        md.append(f"- **Docstrings**: {'Yes' if self.has_docstrings else 'No'}")
        if self.docstring_style:
            md.append(f"- **Docstring Style**: {self.docstring_style}")
        md.append(f"- **Type Hints/Annotations**: {'Yes' if self.has_type_hints else 'No'}")
        md.append("")

        # Testing
        if self.test_framework:
            md.append("## Testing")
            md.append("")
            md.append(f"- **Framework**: {self.test_framework}")
            if self.test_naming_pattern:
                md.append(f"- **Test Naming**: `{self.test_naming_pattern}`")
            md.append("")

        # Detected rules
        if self.rules:
            md.append("## Detected Patterns")
            md.append("")

            # Group by category
            by_category: Dict[str, List[ConventionRule]] = {}
            for rule in self.rules:
                if rule.category not in by_category:
                    by_category[rule.category] = []
                by_category[rule.category].append(rule)

            for category, rules in by_category.items():
                md.append(f"### {category}")
                md.append("")
                for rule in rules:
                    confidence_bar = "[" + "#" * int(rule.confidence * 10) + "-" * (10 - int(rule.confidence * 10)) + "]"
                    md.append(f"- {rule.rule} {confidence_bar}")
                    if rule.examples:
                        md.append(f"  - Examples: `{rule.examples[0]}`")
                md.append("")

        md.append("---")
        md.append("*Review and adjust these conventions as needed before committing.*")

        return "\n".join(md)


class ConventionsAnalyzer:
    """
    Analyzes codebase to detect coding conventions
    """

    # Common source directories
    SOURCE_DIRS = {"src", "lib", "app", "source", "main", "core"}
    TEST_DIRS = {"test", "tests", "__tests__", "spec", "specs", "test_"}

    # File extension to language mapping
    EXTENSIONS = {
        ".java": "java",
        ".py": "python",
        ".ts": "typescript",
        ".tsx": "typescript",
        ".js": "javascript",
        ".jsx": "javascript",
        ".go": "go",
        ".rs": "rust",
        ".cs": "csharp",
        ".rb": "ruby",
        ".php": "php",
        ".kt": "kotlin",
        ".scala": "scala",
    }

    def __init__(self, project_root: str):
        """
        Initialize analyzer

        Args:
            project_root: Root directory of the project
        """
        self.project_root = Path(project_root)
        self.files_analyzed = 0
        self.code_samples: Dict[str, List[str]] = {}

    def analyze(self) -> ProjectConventions:
        """
        Analyze project and detect conventions

        Returns:
            ProjectConventions object with detected patterns
        """
        # Detect primary language
        language = self._detect_primary_language()

        # Detect frameworks
        frameworks = self._detect_frameworks()

        # Detect project type
        project_type = self._detect_project_type(language, frameworks)

        # Create conventions object
        conventions = ProjectConventions(
            project_type=project_type,
            language=language,
            frameworks=frameworks,
            rules=[]
        )

        # Analyze naming conventions
        self._analyze_naming_conventions(conventions)

        # Analyze file organization
        self._analyze_file_organization(conventions)

        # Analyze documentation patterns
        self._analyze_documentation(conventions)

        # Analyze testing patterns
        self._analyze_testing(conventions)

        # Analyze error handling
        self._analyze_error_handling(conventions)

        # Detect framework-specific conventions
        self._analyze_framework_conventions(conventions)

        return conventions

    def _detect_primary_language(self) -> str:
        """Detect the primary programming language"""
        extension_counts: Counter = Counter()

        for root, dirs, files in os.walk(self.project_root):
            # Skip common non-source directories
            dirs[:] = [d for d in dirs if d not in {
                "node_modules", "venv", ".git", "__pycache__",
                "dist", "build", "target", ".idea", ".vscode"
            }]

            for file in files:
                ext = Path(file).suffix.lower()
                if ext in self.EXTENSIONS:
                    extension_counts[ext] += 1

        if not extension_counts:
            return "unknown"

        most_common = extension_counts.most_common(1)[0][0]
        return self.EXTENSIONS.get(most_common, "unknown")

    def _detect_frameworks(self) -> List[str]:
        """Detect frameworks used in the project"""
        frameworks = []

        # Check package.json for Node.js
        package_json = self.project_root / "package.json"
        if package_json.exists():
            try:
                content = json.loads(package_json.read_text())
                deps = {
                    **content.get("dependencies", {}),
                    **content.get("devDependencies", {})
                }

                if "react" in deps:
                    frameworks.append("React")
                if "next" in deps:
                    frameworks.append("Next.js")
                if "express" in deps:
                    frameworks.append("Express")
                if "nestjs" in deps or "@nestjs/core" in deps:
                    frameworks.append("NestJS")
                if "vue" in deps:
                    frameworks.append("Vue")
                if "angular" in deps or "@angular/core" in deps:
                    frameworks.append("Angular")
            except (json.JSONDecodeError, IOError):
                pass

        # Check requirements.txt / pyproject.toml for Python
        requirements = self.project_root / "requirements.txt"
        if requirements.exists():
            content = requirements.read_text().lower()
            if "fastapi" in content:
                frameworks.append("FastAPI")
            if "django" in content:
                frameworks.append("Django")
            if "flask" in content:
                frameworks.append("Flask")
            if "pytest" in content:
                frameworks.append("pytest")

        # Check pom.xml for Java
        pom = self.project_root / "pom.xml"
        if pom.exists():
            content = pom.read_text()
            if "spring-boot" in content:
                frameworks.append("Spring Boot")
            if "spring-cloud" in content:
                frameworks.append("Spring Cloud")
            if "junit" in content.lower():
                frameworks.append("JUnit")

        # Check build.gradle
        gradle = self.project_root / "build.gradle"
        if gradle.exists():
            content = gradle.read_text()
            if "spring" in content.lower():
                frameworks.append("Spring Boot")

        return frameworks

    def _detect_project_type(self, language: str, frameworks: List[str]) -> str:
        """Detect project type based on language and frameworks"""
        framework_lower = [f.lower() for f in frameworks]

        if language == "java":
            if "spring boot" in framework_lower:
                return "Java Spring Boot"
            return "Java"
        elif language == "python":
            if "fastapi" in framework_lower:
                return "Python FastAPI"
            if "django" in framework_lower:
                return "Python Django"
            if "flask" in framework_lower:
                return "Python Flask"
            return "Python"
        elif language == "typescript":
            if "next.js" in framework_lower:
                return "TypeScript Next.js"
            if "react" in framework_lower:
                return "TypeScript React"
            if "nestjs" in framework_lower:
                return "TypeScript NestJS"
            return "TypeScript"
        elif language == "javascript":
            if "react" in framework_lower:
                return "JavaScript React"
            if "express" in framework_lower:
                return "JavaScript Express"
            return "JavaScript"
        elif language == "go":
            return "Go"
        elif language == "rust":
            return "Rust"

        return f"{language.capitalize()}"

    def _analyze_naming_conventions(self, conventions: ProjectConventions):
        """Analyze naming conventions in the codebase"""
        variable_names: List[str] = []
        function_names: List[str] = []
        class_names: List[str] = []
        constant_names: List[str] = []
        file_names: List[str] = []

        for root, dirs, files in os.walk(self.project_root):
            dirs[:] = [d for d in dirs if d not in {
                "node_modules", "venv", ".git", "__pycache__",
                "dist", "build", "target"
            }]

            for file in files:
                file_path = Path(root) / file
                ext = file_path.suffix.lower()

                # Collect file names
                if ext in self.EXTENSIONS:
                    file_names.append(file_path.stem)

                    try:
                        content = file_path.read_text(encoding='utf-8', errors='ignore')

                        # Extract names based on language
                        if ext in {".py"}:
                            self._extract_python_names(
                                content, variable_names, function_names,
                                class_names, constant_names
                            )
                        elif ext in {".java", ".kt"}:
                            self._extract_java_names(
                                content, variable_names, function_names,
                                class_names, constant_names
                            )
                        elif ext in {".ts", ".tsx", ".js", ".jsx"}:
                            self._extract_js_names(
                                content, variable_names, function_names,
                                class_names, constant_names
                            )
                    except Exception:
                        pass

        # Detect naming styles
        conventions.naming_style_variables = self._detect_naming_style(variable_names)
        conventions.naming_style_functions = self._detect_naming_style(function_names)
        conventions.naming_style_classes = self._detect_naming_style(class_names)
        conventions.naming_style_constants = self._detect_naming_style(constant_names)
        conventions.naming_style_files = self._detect_naming_style(file_names)

        # Add rules
        if conventions.naming_style_variables != NamingStyle.UNKNOWN:
            conventions.rules.append(ConventionRule(
                category="Naming",
                rule=f"Variables use {conventions.naming_style_variables.value}",
                confidence=0.8,
                examples=variable_names[:3]
            ))

    def _extract_python_names(
        self, content: str,
        variables: List[str], functions: List[str],
        classes: List[str], constants: List[str]
    ):
        """Extract names from Python code"""
        # Functions
        for match in re.finditer(r'^def\s+(\w+)\s*\(', content, re.MULTILINE):
            functions.append(match.group(1))

        # Classes
        for match in re.finditer(r'^class\s+(\w+)', content, re.MULTILINE):
            classes.append(match.group(1))

        # Variables (simple assignment at module level)
        for match in re.finditer(r'^(\w+)\s*=\s*(?!.*def|class)', content, re.MULTILINE):
            name = match.group(1)
            if name.isupper():
                constants.append(name)
            elif not name.startswith('_'):
                variables.append(name)

    def _extract_java_names(
        self, content: str,
        variables: List[str], functions: List[str],
        classes: List[str], constants: List[str]
    ):
        """Extract names from Java code"""
        # Classes
        for match in re.finditer(r'class\s+(\w+)', content):
            classes.append(match.group(1))

        # Methods
        for match in re.finditer(
            r'(?:public|private|protected)\s+(?:static\s+)?(?:\w+(?:<[^>]+>)?)\s+(\w+)\s*\(',
            content
        ):
            functions.append(match.group(1))

        # Constants (static final)
        for match in re.finditer(
            r'static\s+final\s+\w+\s+(\w+)\s*=',
            content
        ):
            constants.append(match.group(1))

    def _extract_js_names(
        self, content: str,
        variables: List[str], functions: List[str],
        classes: List[str], constants: List[str]
    ):
        """Extract names from JavaScript/TypeScript code"""
        # Functions
        for match in re.finditer(r'function\s+(\w+)\s*\(', content):
            functions.append(match.group(1))

        # Arrow functions assigned to const
        for match in re.finditer(r'const\s+(\w+)\s*=\s*(?:async\s*)?\([^)]*\)\s*=>', content):
            functions.append(match.group(1))

        # Classes
        for match in re.finditer(r'class\s+(\w+)', content):
            classes.append(match.group(1))

        # Constants (SCREAMING_CASE)
        for match in re.finditer(r'const\s+([A-Z][A-Z0-9_]+)\s*=', content):
            constants.append(match.group(1))

        # Variables
        for match in re.finditer(r'(?:let|var|const)\s+(\w+)\s*=', content):
            name = match.group(1)
            if not name.isupper():
                variables.append(name)

    def _detect_naming_style(self, names: List[str]) -> NamingStyle:
        """Detect the predominant naming style from a list of names"""
        if not names:
            return NamingStyle.UNKNOWN

        style_counts = Counter()

        for name in names:
            if name.isupper() and '_' in name:
                style_counts[NamingStyle.SCREAMING_SNAKE] += 1
            elif '_' in name and name.islower():
                style_counts[NamingStyle.SNAKE_CASE] += 1
            elif '-' in name:
                style_counts[NamingStyle.KEBAB_CASE] += 1
            elif name[0].isupper() and not name.isupper():
                style_counts[NamingStyle.PASCAL_CASE] += 1
            elif name[0].islower() and any(c.isupper() for c in name):
                style_counts[NamingStyle.CAMEL_CASE] += 1

        if not style_counts:
            return NamingStyle.UNKNOWN

        return style_counts.most_common(1)[0][0]

    def _analyze_file_organization(self, conventions: ProjectConventions):
        """Analyze file organization patterns"""
        source_dirs = []
        test_dirs = []
        config_files = []

        for item in self.project_root.iterdir():
            if item.is_dir():
                name_lower = item.name.lower()
                if name_lower in self.SOURCE_DIRS:
                    source_dirs.append(item.name)
                elif name_lower in self.TEST_DIRS or name_lower.startswith("test"):
                    test_dirs.append(item.name)
            elif item.is_file():
                if item.suffix in {".yml", ".yaml", ".json", ".toml", ".ini"}:
                    config_files.append(item.name)
                elif item.name.startswith(".") and not item.name.startswith(".git"):
                    config_files.append(item.name)

        conventions.source_directories = source_dirs
        conventions.test_directories = test_dirs
        conventions.config_files = config_files

    def _analyze_documentation(self, conventions: ProjectConventions):
        """Analyze documentation patterns"""
        docstring_count = 0
        type_hint_count = 0
        files_checked = 0

        docstring_styles = Counter()

        for root, dirs, files in os.walk(self.project_root):
            dirs[:] = [d for d in dirs if d not in {
                "node_modules", "venv", ".git", "__pycache__"
            }]

            for file in files:
                if not file.endswith(('.py', '.ts', '.js', '.java')):
                    continue

                file_path = Path(root) / file
                try:
                    content = file_path.read_text(encoding='utf-8', errors='ignore')
                    files_checked += 1

                    # Check for docstrings/JSDoc
                    if '"""' in content or "'''" in content:
                        docstring_count += 1
                        # Detect style
                        if "Args:" in content or "Returns:" in content:
                            docstring_styles["google"] += 1
                        elif "Parameters" in content and "----------" in content:
                            docstring_styles["numpy"] += 1
                        elif ":param" in content or ":returns:" in content:
                            docstring_styles["sphinx"] += 1

                    if "/**" in content and "*/" in content:
                        docstring_count += 1
                        docstring_styles["jsdoc"] += 1

                    # Check for type hints
                    if file.endswith('.py'):
                        if re.search(r'def\s+\w+\([^)]*:\s*\w+', content):
                            type_hint_count += 1
                    elif file.endswith(('.ts', '.tsx')):
                        type_hint_count += 1  # TypeScript always has types

                except Exception:
                    pass

        conventions.has_docstrings = docstring_count > files_checked * 0.3
        conventions.has_type_hints = type_hint_count > files_checked * 0.3

        if docstring_styles:
            conventions.docstring_style = docstring_styles.most_common(1)[0][0]

    def _analyze_testing(self, conventions: ProjectConventions):
        """Analyze testing patterns"""
        # Detect test framework
        if "pytest" in conventions.frameworks:
            conventions.test_framework = "pytest"
            conventions.test_naming_pattern = "test_*.py / test_*"
        elif "JUnit" in conventions.frameworks:
            conventions.test_framework = "JUnit"
            conventions.test_naming_pattern = "*Test.java / @Test methods"
        elif (self.project_root / "jest.config.js").exists():
            conventions.test_framework = "Jest"
            conventions.test_naming_pattern = "*.test.ts / *.spec.ts"
        elif (self.project_root / "package.json").exists():
            try:
                pkg = json.loads((self.project_root / "package.json").read_text())
                if "jest" in pkg.get("devDependencies", {}):
                    conventions.test_framework = "Jest"
                elif "mocha" in pkg.get("devDependencies", {}):
                    conventions.test_framework = "Mocha"
            except Exception:
                pass

    def _analyze_error_handling(self, conventions: ProjectConventions):
        """Analyze error handling patterns"""
        patterns = Counter()

        for root, dirs, files in os.walk(self.project_root):
            dirs[:] = [d for d in dirs if d not in {
                "node_modules", "venv", ".git"
            }]

            for file in files:
                ext = Path(file).suffix
                if ext not in self.EXTENSIONS:
                    continue

                try:
                    content = (Path(root) / file).read_text(errors='ignore')

                    # Python patterns
                    if "raise HTTPException" in content:
                        patterns["FastAPI HTTPException"] += 1
                    if "raise Exception" in content:
                        patterns["Generic Exception"] += 1
                    if "except Exception as e:" in content:
                        patterns["Catch-all handler"] += 1

                    # Java patterns
                    if "@ExceptionHandler" in content:
                        patterns["Spring @ExceptionHandler"] += 1
                    if "throw new" in content:
                        patterns["Explicit exceptions"] += 1

                    # JS/TS patterns
                    if "throw new Error" in content:
                        patterns["Thrown Error"] += 1
                    if ".catch(" in content:
                        patterns["Promise catch"] += 1
                    if "try {" in content:
                        patterns["Try-catch blocks"] += 1

                except Exception:
                    pass

        if patterns:
            conventions.error_handling_pattern = patterns.most_common(1)[0][0]

    def _analyze_framework_conventions(self, conventions: ProjectConventions):
        """Analyze framework-specific conventions"""
        if "Spring Boot" in conventions.frameworks:
            self._analyze_spring_conventions(conventions)
        if "FastAPI" in conventions.frameworks:
            self._analyze_fastapi_conventions(conventions)
        if "React" in conventions.frameworks:
            self._analyze_react_conventions(conventions)

    def _analyze_spring_conventions(self, conventions: ProjectConventions):
        """Analyze Spring Boot specific conventions"""
        rules = []

        # Check for common Spring patterns
        src_main = self.project_root / "src" / "main" / "java"
        if src_main.exists():
            has_controller = False
            has_service = False
            has_repository = False

            for java_file in src_main.rglob("*.java"):
                content = java_file.read_text(errors='ignore')
                if "@RestController" in content or "@Controller" in content:
                    has_controller = True
                if "@Service" in content:
                    has_service = True
                if "@Repository" in content:
                    has_repository = True

            if has_controller and has_service and has_repository:
                rules.append(ConventionRule(
                    category="Architecture",
                    rule="Follows Controller-Service-Repository pattern",
                    confidence=0.9,
                    examples=["*Controller.java", "*Service.java", "*Repository.java"]
                ))

        conventions.rules.extend(rules)

    def _analyze_fastapi_conventions(self, conventions: ProjectConventions):
        """Analyze FastAPI specific conventions"""
        rules = []

        # Check for common FastAPI patterns
        for py_file in self.project_root.rglob("*.py"):
            try:
                content = py_file.read_text(errors='ignore')

                if "@app.get" in content or "@router.get" in content:
                    rules.append(ConventionRule(
                        category="API",
                        rule="Uses FastAPI route decorators",
                        confidence=0.9,
                        examples=["@app.get", "@router.post"]
                    ))
                    break

                if "Depends(" in content:
                    rules.append(ConventionRule(
                        category="Dependency Injection",
                        rule="Uses FastAPI Depends for DI",
                        confidence=0.85,
                        examples=["Depends(get_db)"]
                    ))
                    break
            except Exception:
                pass

        conventions.rules.extend(rules)

    def _analyze_react_conventions(self, conventions: ProjectConventions):
        """Analyze React specific conventions"""
        rules = []

        # Check for hooks usage
        hooks_found = False
        functional_components = 0
        class_components = 0

        for tsx_file in self.project_root.rglob("*.tsx"):
            try:
                content = tsx_file.read_text(errors='ignore')

                if "useState" in content or "useEffect" in content:
                    hooks_found = True

                if re.search(r'const\s+\w+\s*[=:].*(React\.FC|FC<|=>.*return)', content):
                    functional_components += 1
                if "extends React.Component" in content or "extends Component" in content:
                    class_components += 1
            except Exception:
                pass

        if hooks_found:
            rules.append(ConventionRule(
                category="React",
                rule="Uses React Hooks (useState, useEffect)",
                confidence=0.9,
                examples=["useState", "useEffect", "useContext"]
            ))

        if functional_components > class_components:
            rules.append(ConventionRule(
                category="React",
                rule="Prefers functional components over class components",
                confidence=0.85,
                examples=["const Component: React.FC = () => {}"]
            ))

        conventions.rules.extend(rules)


def analyze_project(project_root: str) -> ProjectConventions:
    """
    Convenience function to analyze a project

    Args:
        project_root: Path to project root

    Returns:
        ProjectConventions object
    """
    analyzer = ConventionsAnalyzer(project_root)
    return analyzer.analyze()


def save_conventions(conventions: ProjectConventions, output_path: str, merge: bool = True):
    """
    Save conventions to file, optionally merging with existing content.

    IMPORTANT: This function respects existing conventions files and only adds
    auto-detected sections. User customizations are ALWAYS preserved.

    Args:
        conventions: ProjectConventions object
        output_path: Path to save markdown file
        merge: If True (default), merge with existing file instead of overwriting
    """
    output_file = Path(output_path)
    new_content = conventions.to_markdown()

    if output_file.exists() and merge:
        existing_content = output_file.read_text(encoding='utf-8', errors='ignore')

        # Create backup before any modification
        backup_path = output_file.with_suffix(output_file.suffix + ".backup")
        backup_path.write_text(existing_content)
        print(f"  [Backup] Created: {backup_path}")

        # Check if this is a user-customized file (not just auto-generated)
        is_auto_generated = "*Auto-generated by Adaptive RAG System*" in existing_content
        has_user_sections = _has_user_customizations(existing_content)

        if has_user_sections or not is_auto_generated:
            # Merge: only add sections that don't exist
            merged_content = _merge_conventions_content(existing_content, new_content)
            output_file.write_text(merged_content)
            print(f"  [Merged] Updated: {output_path} (preserved existing customizations)")
        else:
            # Pure auto-generated file, safe to update
            output_file.write_text(new_content)
            print(f"  [Updated] Replaced auto-generated: {output_path}")
    else:
        # No existing file, create new
        output_file.parent.mkdir(parents=True, exist_ok=True)
        output_file.write_text(new_content)
        print(f"  [Created] New file: {output_path}")


def _has_user_customizations(content: str) -> bool:
    """
    Check if a conventions file has user customizations.

    Args:
        content: Existing file content

    Returns:
        True if file appears to have user-added content
    """
    # Markers that indicate user customization
    user_markers = [
        "# Custom",
        "## Custom",
        "# User",
        "## User",
        "# Team",
        "## Team",
        "# Project-Specific",
        "# Additional",
        "## Additional",
        "# Override",
        "IMPORTANT:",
        "NOTE:",
        "TODO:",
    ]

    content_lower = content.lower()
    return any(marker.lower() in content_lower for marker in user_markers)


def _merge_conventions_content(existing: str, new_content: str) -> str:
    """
    Merge new auto-detected conventions with existing content.

    Strategy:
    - Keep ALL existing content
    - Only add new sections that don't already exist
    - Add new content in a clearly marked "Auto-Updated" section

    Args:
        existing: Existing file content
        new_content: New auto-detected content

    Returns:
        Merged content
    """
    # Parse sections from both files
    existing_sections = _parse_markdown_sections(existing)
    new_sections = _parse_markdown_sections(new_content)

    # Find sections that exist in new but not in existing
    existing_headers = {h.lower() for h in existing_sections.keys()}
    missing_sections = []

    for header, content in new_sections.items():
        # Skip the title and metadata
        if header.startswith("# Project Coding Conventions"):
            continue
        if "*Auto-generated" in header:
            continue

        header_lower = header.lower()
        # Check if similar section exists
        if not any(h in header_lower or header_lower in h for h in existing_headers):
            missing_sections.append((header, content))

    # If no new sections to add, return existing as-is
    if not missing_sections:
        return existing

    # Append new sections
    result = existing.rstrip()
    result += "\n\n"
    result += "---\n"
    result += "# Auto-Detected Updates\n"
    result += "*The following sections were auto-detected and may be useful to review:*\n\n"

    for header, content in missing_sections:
        result += f"{header}\n{content}\n\n"

    result += "---\n"
    result += "*Review and integrate these detected patterns into your conventions above.*\n"

    return result


def _parse_markdown_sections(content: str) -> dict:
    """
    Parse markdown content into sections by header.

    Args:
        content: Markdown content

    Returns:
        Dict mapping header text to section content
    """
    sections = {}
    current_header = ""
    current_content = []

    for line in content.split('\n'):
        if line.startswith('#'):
            # Save previous section
            if current_header:
                sections[current_header] = '\n'.join(current_content)
            current_header = line
            current_content = []
        else:
            current_content.append(line)

    # Don't forget the last section
    if current_header:
        sections[current_header] = '\n'.join(current_content)

    return sections


if __name__ == "__main__":
    import sys
    import argparse

    parser = argparse.ArgumentParser(
        description="Analyze codebase for coding conventions"
    )
    parser.add_argument(
        "project",
        nargs="?",
        default=".",
        help="Path to project root (default: current directory)"
    )
    parser.add_argument(
        "--no-merge",
        action="store_true",
        help="Overwrite existing conventions file instead of merging"
    )
    parser.add_argument(
        "--output", "-o",
        help="Custom output path (default: PROJECT/.claude/CONVENTIONS.md)"
    )
    parser.add_argument(
        "--json",
        action="store_true",
        help="Output JSON instead of markdown"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Analyze but don't save (just print results)"
    )

    args = parser.parse_args()

    print(f"[*] Analyzing project: {args.project}")
    conventions = analyze_project(args.project)

    if args.json:
        print(json.dumps(conventions.to_dict(), indent=2))
    else:
        print("\n" + conventions.to_markdown())

    if not args.dry_run:
        # Determine output path
        if args.output:
            output_file = Path(args.output)
        else:
            output_file = Path(args.project) / ".claude" / "CONVENTIONS.md"

        output_file.parent.mkdir(parents=True, exist_ok=True)

        merge = not args.no_merge
        print(f"\n[*] Saving conventions (merge={merge})...")
        save_conventions(conventions, str(output_file), merge=merge)
        print(f"[OK] Saved to: {output_file}")
    else:
        print("\n[*] Dry run - no file saved")
